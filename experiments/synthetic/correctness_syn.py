# -*- coding: utf-8 -*-
"""QuantitativeEvaluation - Playground.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kEoqH20gWonAYJ1YBCgkdKuYRq96FUW3

# Import libraries, models, data
"""

import os
import random
import sys
from tqdm import tqdm
import numpy as np
import pandas as pd
from sklearn import metrics
import torch
import matplotlib.pyplot as plt


random.seed(123)
torch.manual_seed(123)
np.random.seed(123)

dtype = torch.float

# Check whether a GPU is available
if torch.cuda.is_available():
    device = torch.device("cuda")     
else:
    device = torch.device("cpu")

print(device)

sys.path.insert(1, '/local/work/enguyen')
from CoreSNN import *
from ExplanationCreationGeneral import *
from ExplanationEvaluationNEW import *

# Load data
dataset = load_obj('/local/work/enguyen/data/syn_data.pkl')

# Fixed parameters, nbsteps and max time correspond to the set duration (for testing, first we consider the validation set and not the test set)
nb_inputs  = 3
nb_outputs = 4

"""# Correctness

Correctness is measured in explanation selectivity. This is the average AUC of the graphs resulting from flippinig the most important feature segments of the explanation.
"""
def initiate_model(nb_layers, t):
    """
    Function that initiates a SNN model with nb_layers which runs data of duration t, only defined for 3 layers
    :param nb_layers: (int) to define the number of layers
    :param t: max_time and nb_steps is defined by this (int)
    """
    nb_inputs = 3
    nb_outputs = 4
    if nb_layers == 1:
        params_onelayer = {'time_step': 0.001,
               'tau_syn': 0.01,
               'tau_mem': 0.001,
               'optimizer': optim.Adam,
               'learning_rate': 0.01,
               'batch_size': 128}
        model = SNN(hyperparams=params_onelayer, 
                          nb_inputs=nb_inputs, 
                          nb_outputs=nb_outputs, 
                          nb_layers=1, 
                          nb_steps=t, 
                          max_time=t)

        model.inference('/local/work/enguyen/synthetic/one_weights.pt')
        return model
    elif nb_layers == 2:
        params_twolayer = {'time_step': 0.001,
               'tau_syn': 0.01,
               'tau_mem': 0.001,
               'optimizer': optim.Adam,
               'learning_rate': 0.01,
               'batch_size': 128,
               'nb_hiddens': [10]}
        model = SNN(hyperparams=params_twolayer, 
                          nb_inputs=nb_inputs, 
                          nb_outputs=nb_outputs, 
                          nb_layers=2, 
                          nb_steps=t, 
                          max_time=t)

        model.inference('/local/work/enguyen/synthetic/two_weights.pt')
        return model
    elif nb_layers == 3:
        params_threelayer = {'time_step': 0.001,
               'tau_syn': 0.01,
               'tau_mem': 0.001,
               'optimizer': optim.Adam,
               'learning_rate': 0.01,
               'batch_size': 128,
               'nb_hiddens': [10, 10]}
        model = SNN(hyperparams=params_threelayer, 
                          nb_inputs=nb_inputs, 
                          nb_outputs=nb_outputs, 
                          nb_layers=3, 
                          nb_steps=t, 
                          max_time=t)
        model.inference('/local/work/enguyen/synthetic/three_weights.pt')
        return model



def flip_segment(X_spikes, segment): 
    """
    Flips the values of a segment in X_spikes format
    """
    _, (d, t_start, t_end) = segment
    X_perturbed = X_spikes.to_dense()
    X_perturbed[:, t_start:t_end, d] = torch.abs(X_perturbed[:, t_start:t_end, d]-1) 
    X_perturbed = X_perturbed.to_sparse()
    return X_perturbed


def flip_and_predict(nb_layers, X_data, y_data, model_explanations, testset_t): 
    """
    Function to get the predictions of the model with nb_layers on X_data when flipping the feature segments
    :param nb_layers: number of layers 
    """
    # define the model for the specific duration (performs same though because the weights are the same)
    model = initiate_model(nb_layers, 1) 
    # for storing the predictions with flipped segments
    # Will be an array of duration arrays of each the length of how many segments they have.
    y_preds_flipped = []
    y_preds = []
    #go through the "samples"
    for t in tqdm(testset_t):
        e, prediction = model_explanations[t]
        y_preds.append(prediction)
        start_t = t-1000 if t>=1000 else 0
        
        model.nb_steps = t-start_t
        model.max_time = t-start_t
        
        # get the relevant part of the dataset, this is done for performance reasons
        X = {'times': X_data['times'][:, np.where((X_data['times']>=start_t) & (X_data['times']<t))[1]]-start_t, 
             'units': X_data['units'][:, np.where((X_data['times']>=start_t) & (X_data['times']<t))[1]]}
        y = y_data[:, start_t:t]
        data_generator = sparse_data_generator_from_spikes(X, y, len(y), model.nb_steps, model.layer_sizes[0], model.max_time, shuffle=False)
        X_spikes, _ = next(data_generator)

        # idenfity feature segments in e that are positively or negatively attributing
        feature_segments = segment_features(e)

        # rank the segments
        ranked_fs = rank_segments(e, feature_segments)
        
        y_pred_perturbed = []
        X_perturbed = X_spikes
        for i, segment in enumerate(ranked_fs): 
            X_perturbed = flip_segment(X_perturbed, segment)

            # Evaluate & record y_pred for the perturbed input
            pred_perturbed, _, _ = predict_from_spikes(model, X_perturbed)
            y_pred_perturbed.append(pred_perturbed[0][-1])
        y_preds_flipped.append(y_pred_perturbed)
    return y_preds_flipped, y_preds


def explanation_selectivity_score(y_preds, y_perturbed_preds, y_true):
    """
    Computes explanation selectivity score as AUC of iteratively flipping the most important feature segment
    i.e., the most important feature segment is flipped for each duration, then the performance is computed, then the 2 most important segments are flipped and performance recorded
    then the AUC is computed by the graph resulting from x-axis of # of segments flipped, and y-axis as the balanced accuracy
    :param y_preds: the original model predictions
    :param y_perturbed_preds: the model predictions with flipped feature segments
    :param y_true: ground truth labels
    :returns: explanation selectivity score (ess) and the performance recordings per number of feature segments flipped
    """
    max_n_segments = max([len(pred) for pred in y_perturbed_preds])
    for pred in y_perturbed_preds:
        pred.extend([pred[-1]]*(max_n_segments-len(pred))) 
    df_pred = pd.DataFrame(y_perturbed_preds) #shape (#samples(same as duration), #feature segments (can be different per sample))
    performances = [balanced_accuracy_score(y_true[0], y_preds)]
    for n_segments_flipped in range(df_pred.shape[1]):
        performances.append(balanced_accuracy_score(y_true[0], df_pred[n_segments_flipped]))
    score = metrics.auc(range(len(performances)), performances)
    return score, performances


with torch.no_grad():
    testset_t = load_obj('/local/work/enguyen/data/expl_syn_testset.pkl')
    y_true = dataset['y_test'][:, testset_t]
    
    expl_path = '/local/work/enguyen/nocw/'

    expl_types = ['ns2', 's']
    for expl_type in expl_types: 
        # OneLayer
        model_explanations = load_obj(expl_path+'expl_one_syn_nocw_'+expl_type+'.pkl')
        y_preds_perturbed, y_preds_clean = flip_and_predict(1, dataset['X_test'], dataset['y_test'], model_explanations, testset_t)
        save_obj(y_preds_perturbed, expl_path+'correctness/syn/'+expl_type+'/y_preds_perturbed_onelayer.pkl')

        # TwoLayer
        model_explanations = load_obj(expl_path+'expl_two_syn_nocw_'+expl_type+'.pkl')
        y_preds_perturbed, y_preds_clean = flip_and_predict(2, dataset['X_test'], dataset['y_test'], model_explanations, testset_t)
        save_obj(y_preds_perturbed, expl_path+'correctness/syn/'+expl_type+'/y_preds_perturbed_twolayer.pkl')

        # ThreeLayer
        model_explanations = load_obj(expl_path+'expl_three_syn_nocw_'+expl_type+'.pkl')
        y_preds_perturbed, y_preds_clean = flip_and_predict(3, dataset['X_test'], dataset['y_test'], model_explanations, testset_t)
        save_obj(y_preds_perturbed, expl_path+'correctness/syn/'+expl_type+'/y_preds_perturbed_threelayer.pkl')
   
