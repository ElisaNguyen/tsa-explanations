# -*- coding: utf-8 -*-
"""QuantitativeEvaluation - Playground.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kEoqH20gWonAYJ1YBCgkdKuYRq96FUW3

# Import libraries, models, data
"""

import os
import random
import sys

import numpy as np
import pandas as pd
from sklearn import metrics
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm

random.seed(123)
torch.manual_seed(123)
np.random.seed(123)

dtype = torch.float

# Check whether a GPU is available
if torch.cuda.is_available():
    device = torch.device("cuda")     
else:
    device = torch.device("cpu")
print(device)

sys.path.insert(1, '/local/work/enguyen')
from CoreSNN import *
from ExplanationCreationGeneral import *
from ExplanationEvaluationNEW import *

# Load data
dataset = load_obj('/local/work/enguyen/data/syn_data.pkl')
nb_inputs = 3
nb_outputs = 4

"""# Attribution sufficiency

Attribution sufficiency describes the sufficiency of the as highly attributing identified features/part of the data for the model prediction. It is measured in the drop of model performance on the dataset when shuffling the unimportant features.

Definition here: attribution values > 0 of the feature segments are important.
"""

def initiate_model(nb_layers, t):
    """
    Function that initiates a SNN model with nb_layers which runs data of duration t, only defined for 3 layers
    :param nb_layers: (int) to define the number of layers
    :param t: max_time and nb_steps is defined by this (int)
    """
    nb_inputs = 3
    nb_outputs = 4
    if nb_layers == 1:
        params_onelayer = {'time_step': 0.001,
               'tau_syn': 0.01,
               'tau_mem': 0.001,
               'optimizer': optim.Adam,
               'learning_rate': 0.01,
               'batch_size': 128}
        model = SNN(hyperparams=params_onelayer, 
                          nb_inputs=nb_inputs, 
                          nb_outputs=nb_outputs, 
                          nb_layers=1, 
                          nb_steps=t, 
                          max_time=t)

        model.inference('/local/work/enguyen/synthetic/one_weights.pt')
        return model
    elif nb_layers == 2:
        params_twolayer = {'time_step': 0.001,
               'tau_syn': 0.01,
               'tau_mem': 0.001,
               'optimizer': optim.Adam,
               'learning_rate': 0.01,
               'batch_size': 128,
               'nb_hiddens': [10]}
        model = SNN(hyperparams=params_twolayer, 
                          nb_inputs=nb_inputs, 
                          nb_outputs=nb_outputs, 
                          nb_layers=2, 
                          nb_steps=t, 
                          max_time=t)

        model.inference('/local/work/enguyen/synthetic/two_weights.pt')
        return model
    elif nb_layers == 3:
        params_threelayer = {'time_step': 0.001,
               'tau_syn': 0.01,
               'tau_mem': 0.001,
               'optimizer': optim.Adam,
               'learning_rate': 0.01,
               'batch_size': 128,
               'nb_hiddens': [10, 10]}
        model = SNN(hyperparams=params_threelayer, 
                          nb_inputs=nb_inputs, 
                          nb_outputs=nb_outputs, 
                          nb_layers=3, 
                          nb_steps=t, 
                          max_time=t)
        model.inference('/local/work/enguyen/synthetic/three_weights.pt')
        return model



def identify_high_attribution(ranked_fs, theta):
    """
    filters the feature segments to those that are contributing more than theta
    :param ranked_fs: list of feature segments with their mean score
    :param theta: threshold for high attribution
    """
    high_attribution_segments = []
    attributing_sensors = []
    for fs in ranked_fs:
        if np.abs(fs[0])>theta:
            high_attribution_segments.append(fs)
            if fs[1][0] not in attributing_sensors:
                attributing_sensors.append(fs[1][0]) 
    return high_attribution_segments, attributing_sensors


def get_time_segments(sensor, segments):
    """
    Function to extract the start timestamps and end timestamps sorted from early to late from a feature segment
    :param sensor: sensor dimension of segment (int)
    :param segments: feature segments to extract the timestamps for
    :returns: starts and ends in sorted lists
    """
    starts = []
    ends = []
    for _, segment in segments:
        if segment[0] == sensor:
            starts.append(segment[1])
            ends.append(segment[2])
    starts_sorted = [start for start,_ in sorted(zip(starts, ends))]
    ends_sorted = [end for _, end in sorted(zip(starts, ends))]
    return starts_sorted, ends_sorted


def perturb_background(attributing_sensors, X_spikes, segments):
    """
    Function to perturb only the background and not the high attributing segments randomly
    :param attributing_sensors: attributing sensors
    :param X_spikes: spiking input
    :param segments: highly attributing segments
    :returns: perturbed X_spikes
    """
    X_spikes_perturbed = X_spikes.to_dense()[0, :, :].t()
    for sensor in range(nb_inputs):
        if sensor in attributing_sensors:
            starts, ends = get_time_segments(sensor, segments)

            # identify the background data (non high attributing)
            background_data = [X_spikes_perturbed[sensor][:starts[0]]]
            for next_start, end in zip(starts[1:], ends[:-1]):
                background_data.append(X_spikes_perturbed[sensor][end:next_start])
            background_data.append(X_spikes_perturbed[sensor][ends[-1]:])
            background_data = torch.cat(background_data)

            # randomly permute the background
            shuffle_idx = torch.randperm(len(background_data))
            background_data = background_data[shuffle_idx]
            perturbed_data = [background_data[:starts[0]]]
            background_data = background_data[starts[0]:]
            for i in range(len(starts)-1):
                perturbed_data.append(X_spikes_perturbed[sensor][starts[i]:ends[i]])
                perturbed_data.append(background_data[:(starts[i+1]-ends[i])])
                background_data = background_data[(starts[i+1]-ends[i]):]
            perturbed_data.append(X_spikes_perturbed[sensor][starts[-1]:ends[-1]])
            perturbed_data.append(background_data)
            perturbed_data = torch.cat(perturbed_data)
            X_spikes_perturbed[sensor] = perturbed_data
        else:
            shuffle_idx = torch.randperm(X_spikes.shape[1])
            X_spikes_perturbed[sensor] = X_spikes_perturbed[sensor][shuffle_idx]
    X_spikes_perturbed = X_spikes_perturbed.t()
    X_spikes_perturbed = torch.unsqueeze(X_spikes_perturbed, 0)
    return X_spikes_perturbed.to_sparse()


def attribution_sufficiency_score(nb_layers, X_data, y_data, model_explanations, theta, testset_t, y_true):
    """
    Computes the attribution sufficiency score as the balanced accuracy of the model's predictions on data with perturbed background and the ground truth
    :param nb_layers: number of layers of the SNN
    :param model_explanations: extracted explanations and X_spikes dictionary
    :param theta: threshold for high attribution
    :param testset_t: timestamps to test
    :returns: attribution sufficiency score and y_pred_bgperturbed
    """
    y_pred_bgperturbed = []
    y_preds = []
    model = initiate_model(nb_layers, 1)
    for t in tqdm(testset_t):
        start_t = t-1000 if t>=1000 else 0
        model.nb_steps = t - start_t
        model.max_time = t - start_t

        e, prediction = model_explanations[t]
        y_preds.append(prediction)

        # get the relevant part of the dataset, this is done for performance reasons
        X = {'times': X_data['times'][:, np.where((X_data['times']>=start_t) & (X_data['times']<t))[1]]-start_t, 
             'units': X_data['units'][:, np.where((X_data['times']>=start_t) & (X_data['times']<t))[1]]}
        y = y_data[:, start_t:t]
        data_generator = sparse_data_generator_from_spikes(X, y, len(y), model.nb_steps, model.layer_sizes[0], model.max_time, shuffle=False)
        X_spikes, _ = next(data_generator)

        fs = segment_features(e)
        fs_scores = rank_segments(e, fs)
        high_attribution_segments, attributing_sensors = identify_high_attribution(fs_scores, theta)
        X_spikes_perturbed = perturb_background(attributing_sensors, X_spikes, high_attribution_segments)

        y_pred_perturbed_here, _, _ = predict_from_spikes(model, X_spikes_perturbed)
        y_pred_bgperturbed.append(y_pred_perturbed_here[0][-1])
    # perf_perturbedbg = balanced_accuracy_score(y_true[0], y_pred_bgperturbed)
    # score = balanced_accuracy_score(y_true[0], y_preds) - perf_perturbedbg
    # return score, y_pred_bgperturbed
    score = metrics.balanced_accuracy_score(y_preds, y_pred_bgperturbed)
    return score, y_preds, y_pred_bgperturbed


def get_thetas(max_attr):
    """
    Given all explanations of one SNN model across both data subjects, find the thetas at 25, 50 and 75% of the attribution range of positive attributions 
    """
    thetas = [0, (0.05*max_attr), (0.10*max_attr), (0.15*max_attr), (0.20*max_attr), (0.25*max_attr), (0.5*max_attr), (0.75*max_attr)]
    return thetas
    

# concatenate A and B together for one network and find the max
# then test 0, 25% and 75% of the [0, max] interval as thetas
testset_t = load_obj('/local/work/enguyen/data/expl_syn_testset.pkl')
y_true = dataset['y_test'][:, testset_t]

expl_path = '/local/work/enguyen/nocw/'
expl_types = ['ns2','s']
for expl_type in expl_types:
    with torch.no_grad():
        # get thetas
        # ugly but refactor later
        max_attr = get_max_attr(load_obj(expl_path+'expl_one_syn_nocw_'+expl_type+'.pkl'))
        max_attr = max(max_attr, get_max_attr(load_obj(expl_path+'expl_two_syn_nocw_'+expl_type+'.pkl')))
        max_attr = max(max_attr, get_max_attr(load_obj(expl_path+'expl_three_syn_nocw_'+expl_type+'.pkl')))

        min_attr = get_min_attr(load_obj(expl_path+'expl_one_syn_nocw_'+expl_type+'.pkl'))
        min_attr = min(min_attr, get_min_attr(load_obj(expl_path+'expl_two_syn_nocw_'+expl_type+'.pkl')))
        min_attr = min(min_attr, get_min_attr(load_obj(expl_path+'expl_three_syn_nocw_'+expl_type+'.pkl')))

        thetas = get_thetas(max(max_attr, np.abs(min_attr)))

        # # OneLayerSNN
        model_explanations = load_obj(expl_path+'expl_one_syn_nocw_'+expl_type+'.pkl')
        for theta in thetas:
            sufficiency_score = attribution_sufficiency_score(1, dataset['X_test'], dataset['y_test'], model_explanations, theta, testset_t, y_true)
            save_obj(sufficiency_score, expl_path+'completeness/syn/'+expl_type+'/one_sufficiency_theta'+str(theta)+'.pkl')
 
        # # # TwoLayerSNN
        model_explanations = load_obj(expl_path+'expl_two_syn_nocw_'+expl_type+'.pkl')
        for theta in thetas:
            sufficiency_score = attribution_sufficiency_score(2, dataset['X_test'], dataset['y_test'], model_explanations, theta, testset_t, y_true)
            save_obj(sufficiency_score, expl_path+'completeness/syn/'+expl_type+'/two_sufficiency_theta'+str(theta)+'.pkl')

        # ThreeLayerSNN
        model_explanations = load_obj(expl_path+'expl_three_syn_nocw_'+expl_type+'.pkl')
        for theta in thetas:
            sufficiency_score = attribution_sufficiency_score(3, dataset['X_test'], dataset['y_test'], model_explanations, theta, testset_t, y_true)
            save_obj(sufficiency_score, expl_path+'completeness/syn/'+expl_type+'/three_sufficiency_theta'+str(theta)+'.pkl')

 