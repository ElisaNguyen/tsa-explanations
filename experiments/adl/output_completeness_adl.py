# -*- coding: utf-8 -*-
"""QuantitativeEvaluation - Playground.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kEoqH20gWonAYJ1YBCgkdKuYRq96FUW3

# Import libraries, models, data
"""

import os
import random
import sys

import numpy as np
import pandas as pd
from sklearn import metrics
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm

random.seed(123)
torch.manual_seed(123)
np.random.seed(123)

dtype = torch.float

# Check whether a GPU is available
if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

from ExplanationEvaluation import *

sys.path.insert(1, '../../models')
from CoreSNN import *

# Load data & fixed variables
dataset = load_obj('../dataset_max.pkl')
nb_inputs = 14
nb_outputs = 11

"""# Output-completeness

Output-completeness describes the sufficiency of the as highly attributing identified features/
part of the data for the model prediction. 
It is measured in the drop of model performance on the dataset when shuffling the unimportant features.

Definition here: attribution values > 0 of the feature segments are important.
"""


# concatenate A and B together for one network and find the max
# then test 0, 25% and 75% of the [0, max] interval as epsilons
A_testset_t = load_obj('../../data/quantitative_test_t_A.pkl')
B_testset_t = load_obj('../../data/quantitative_test_t_B.pkl')
A_y_true = dataset['y_test_A'][:, A_testset_t]
B_y_true = dataset['y_test_B'][:, B_testset_t]
expl_types = ['s', 'ns', 'sam']

with torch.no_grad():
    for expl_type in expl_types:
        # get epsilons
        max_attr = -50
        min_attr = 50
        for filename in os.listdir('../evaluation/{}'.format(expl_type)):
            f = '../evaluation/{}/{}'.format(expl_type, filename)
            if os.path.isfile(f):
                max_attr = max(max_attr, get_max_attr(f))
                min_attr = min(min_attr, get_min_attr(f))

        epsilons = get_epsilons(max(max_attr, np.abs(min_attr)))

        for nb_layer in range(3):
            # A
            model_explanations = load_obj('../evaluation/{}/{}L_explanations_A.pkl'.format(expl_type, nb_layer))
            for i, epsilon in enumerate(epsilons):
                oc_score = output_completeness_score(nb_layer, dataset['X_test_A'], dataset['y_test_A'],
                                                     model_explanations, epsilon, A_testset_t, A_y_true)
                save_obj(oc_score,
                         '../evaluation/output_completeness/{}/{}L_oc_A_epsilon{}.pkl'.format(expl_type, nb_layer, i))
            # B
            model_explanations = load_obj('../evaluation/{}/{}L_explanations_B.pkl'.format(expl_type, nb_layer))
            for i, epsilon in enumerate(epsilons):
                oc_score = output_completeness_score(nb_layer, dataset['X_test_B'], dataset['y_test_B'],
                                                     model_explanations, epsilon, B_testset_t, B_y_true)
                save_obj(oc_score,
                         '../evaluation/output_completeness/{}/{}L_oc_B_epsilon{}.pkl'.format(expl_type, nb_layer, i))
            print('Evaluation of output-completeness for {} explanations of SNN-{}L done!'.format(expl_type, nb_layer))
