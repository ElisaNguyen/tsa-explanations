{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Quantitative Analysis - Results.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "6rsZZeKfYy3Q"
   ]
  },
  "kernelspec": {
   "name": "pycharm-81b8a9ed",
   "language": "python",
   "display_name": "PyCharm (tsa)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "lh5DZKgeYYr3"
   },
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import balanced_accuracy_score, auc\n",
    "\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "os.chdir('../models/')\n",
    "from CoreSNN import *\n",
    "os.chdir('../')\n",
    "from ExplanationCreation import *\n",
    "from ExplanationEvaluation import *"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gK20VFQVi_5w"
   },
   "source": [
    "dataset = load_obj('../../data/dataset_max.pkl')\n",
    "A_testset_t = load_obj('../../data/quantitative_test_t_A_final.pkl')\n",
    "B_testset_t = load_obj('../../data/quantitative_test_t_B_final.pkl')\n",
    "A_y_true = dataset['y_test_A'][:, A_testset_t]\n",
    "B_y_true = dataset['y_test_B'][:, B_testset_t]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rsZZeKfYy3Q"
   },
   "source": [
    "# Output-completeness"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QqFUZZqhBt1P"
   },
   "source": [
    "os.chdir('') #Add path to results\n",
    "epsilons = {'s': [], 'ns':[], 'sam':[]} # Fill tested epsilon values here\n",
    "\n",
    "def get_oc_scores(nb_layer, expl_type, epsilons):\n",
    "    _, y_pred_A_0, y_pred_p_A_0 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_A_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[0]))\n",
    "    _, y_pred_A_5, y_pred_p_A_5 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_A_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[1]))\n",
    "    _, y_pred_A_10, y_pred_p_A_10 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_A_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[2]))\n",
    "    _, y_pred_A_15, y_pred_p_A_15 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_A_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[3]))\n",
    "    _, y_pred_A_20, y_pred_p_A_20 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_A_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[4]))\n",
    "    _, y_pred_A_25, y_pred_p_A_25 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_A_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[5]))\n",
    "    _, y_pred_A_50, y_pred_p_A_50 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_A_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[6]))\n",
    "    _, y_pred_A_75, y_pred_p_A_75 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_A_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[7]))\n",
    "    _, y_pred_B_0, y_pred_p_B_0 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_B_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[0]))\n",
    "    _, y_pred_B_5, y_pred_p_B_5 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_B_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[1]))\n",
    "    _, y_pred_B_10, y_pred_p_B_10 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_B_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[2]))\n",
    "    _, y_pred_B_15, y_pred_p_B_15 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_B_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[3]))\n",
    "    _, y_pred_B_20, y_pred_p_B_20 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_B_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[4]))\n",
    "    _, y_pred_B_25, y_pred_p_B_25 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_B_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[5]))\n",
    "    _, y_pred_B_50, y_pred_p_B_50 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_B_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[6]))\n",
    "    _, y_pred_B_75, y_pred_p_B_75 = load_obj('..\\\\evaluation\\\\output_completeness\\\\{}\\\\{}L_oc_B_epsilon{}.pkl'.format(expl_type, nb_layer, epsilons[7]))\n",
    "\n",
    "    scores = [balanced_accuracy_score([*y_pred_A_0, *y_pred_B_0], [*y_pred_p_A_0, *y_pred_p_B_0]),\n",
    "              balanced_accuracy_score([*y_pred_A_5, *y_pred_B_5], [*y_pred_p_A_5, *y_pred_p_B_5]),\n",
    "              balanced_accuracy_score([*y_pred_A_10, *y_pred_B_10], [*y_pred_p_A_10, *y_pred_p_B_10]),\n",
    "              balanced_accuracy_score([*y_pred_A_15, *y_pred_B_15], [*y_pred_p_A_15, *y_pred_p_B_15]),\n",
    "              balanced_accuracy_score([*y_pred_A_20, *y_pred_B_20], [*y_pred_p_A_20, *y_pred_p_B_20]),\n",
    "              balanced_accuracy_score([*y_pred_A_25, *y_pred_B_25], [*y_pred_p_A_25, *y_pred_p_B_25]),\n",
    "              balanced_accuracy_score([*y_pred_A_50, *y_pred_B_50], [*y_pred_p_A_50, *y_pred_p_B_50]),\n",
    "              balanced_accuracy_score([*y_pred_A_75, *y_pred_B_75], [*y_pred_p_A_75, *y_pred_p_B_75])]\n",
    "    return scores"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ng2wQD0r1Amz"
   },
   "source": [
    "oc_s = []\n",
    "oc_ns = []\n",
    "oc_sam = []\n",
    "\n",
    "for nb_layer in range(3):\n",
    "    oc_s.append(get_oc_scores(nb_layer, 's', epsilons['s']))\n",
    "    oc_ns.append(get_oc_scores(nb_layer, 'ns', epsilons['ns']))\n",
    "    oc_sam.append(get_oc_scores(nb_layer, 'sam', epsilons['sam']))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRkj604qcPDI"
   },
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Sn8e3Z6UC-nH",
    "outputId": "6aae9d06-519d-4e97-d9f0-c30104d9c97e"
   },
   "source": [
    "fig = plt.figure(tight_layout=True, frameon=False, figsize=(15,5),dpi=200)\n",
    "gs = gridspec.GridSpec(1,3)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax1.plot(oc_s[0])\n",
    "ax1.plot(oc_ns[0])\n",
    "ax1.plot(oc_sam[0], linestyle='dotted')\n",
    "ax1.set_ylim(ymin = 0, ymax=1)\n",
    "ax1.set_ylabel('Output-completeness score', fontdict={'fontsize': 16})\n",
    "ax1.set_xticks(range(4))\n",
    "ax1.set_xticklabels([0, '5%', '10%', '15%', '20%', '25%', '50%', '75%'])\n",
    "ax1.set_xlabel('Epsilon', fontdict={'fontsize': 16})\n",
    "ax1.set_title('Output-completeness for explanations of SNN-1L', pad=10)\n",
    "ax1.legend(['TSA-S','TSA-NS', 'SAM'], prop={'size':13})\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "ax2.plot(oc_s[1])\n",
    "ax2.plot(oc_ns[1])\n",
    "ax2.plot(oc_sam[1], linestyle='dotted')\n",
    "ax2.set_ylim(ymin = 0, ymax=1)\n",
    "ax2.set_ylabel('Output-completeness score', fontdict={'fontsize': 16})\n",
    "ax2.set_xticks(range(4))\n",
    "ax2.set_xticklabels([0, '5%', '10%', '15%', '20%', '25%', '50%', '75%'])\n",
    "ax2.set_xlabel('Epsilon', fontdict={'fontsize': 16})\n",
    "ax2.set_title('Output-completeness for explanations of SNN-2L', pad=10)\n",
    "ax2.legend(['TSA-S','TSA-NS', 'SAM'], prop={'size':13})\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,2])\n",
    "ax3.plot(oc_s[2])\n",
    "ax3.plot(oc_ns[2])\n",
    "ax3.plot(oc_sam[2], linestyle='dotted')\n",
    "ax3.set_ylim(ymin = 0, ymax=1)\n",
    "ax3.set_ylabel('Output-completeness score', fontdict={'fontsize': 16})\n",
    "ax3.set_xticks(range(4))\n",
    "ax3.set_xticklabels([0, '5%', '10%', '15%', '20%', '25%', '50%', '75%'])\n",
    "ax3.set_xlabel('Epsilon', fontdict={'fontsize': 16})\n",
    "ax3.set_title('Output-completeness for explanations of SNN-3L', pad=10)\n",
    "ax3.legend(['TSA-S','TSA-NS', 'SAM'], prop={'size':13})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAsGK0-CtPrB"
   },
   "source": [
    "# Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Qttgr3j1AhS"
   },
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r4m7RQSjtRaZ"
   },
   "source": [
    "y_preds_p_1A_s = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('s', '1L', 'A'))\n",
    "y_preds_p_1B_s = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('s', '1L', 'B'))\n",
    "y_preds_p_2A_s = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('s', '2L', 'A'))\n",
    "y_preds_p_2B_s = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('s', '2L', 'B'))\n",
    "y_preds_p_3A_s = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('s', '3L', 'A'))\n",
    "y_preds_p_3B_s = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('s', '3L', 'B'))\n",
    "\n",
    "y_preds_p_1A_ns = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('ns', '1L', 'A'))\n",
    "y_preds_p_1B_ns = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('ns', '1L', 'B'))\n",
    "y_preds_p_2A_ns = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('ns', '2L', 'A'))\n",
    "y_preds_p_2B_ns = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('ns', '2L', 'B'))\n",
    "y_preds_p_3A_ns = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('ns', '3L', 'A'))\n",
    "y_preds_p_3B_ns = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('ns', '3L', 'B'))\n",
    "\n",
    "y_preds_p_1A_sam = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('sam', '1L', 'A'))\n",
    "y_preds_p_1B_sam = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('sam', '1L', 'B'))\n",
    "y_preds_p_2A_sam = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('sam', '2L', 'A'))\n",
    "y_preds_p_2B_sam = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('sam', '2L', 'B'))\n",
    "y_preds_p_3A_sam = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('sam', '3L', 'A'))\n",
    "y_preds_p_3B_sam = load_obj('..\\\\correctness\\\\{}\\\\y_preds_perturbed_{}_{}.pkl'.format('sam', '3L', 'B'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ikoPcJxsfMpA"
   },
   "source": [
    "# no feature segments found for A t=9 in the explanations extracted from ThreeLayerSNN, so only consider the original prediction \n",
    "y_preds_p_3A_s[37] = [5]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QBtvRgq1Dx3"
   },
   "source": [
    "## Normalization and combination of the results per model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_1PrZEMoN_cj"
   },
   "source": [
    "def get_perf_curve_yhat(y_preds_p_A, y_preds_p_B):\n",
    "    y_hat = [pred[0] for pred in y_preds_p_A]\n",
    "    y_hat_B = [pred[0] for pred in y_preds_p_B]\n",
    "    y_hat.extend(y_hat_B)\n",
    "\n",
    "    perf = []\n",
    "    for i in range(max([len(y_pred) for y_pred in y_preds_p_A])):\n",
    "        y_pred_p = [pred[i] if i<len(pred) else pred[-1] for pred in y_preds_p_A]\n",
    "        y_pred_p_B = [pred[i] if i<len(pred) else pred[-1] for pred in y_preds_p_B]\n",
    "        y_pred_p.extend(y_pred_p_B)\n",
    "        perf.append(balanced_accuracy_score(y_hat, y_pred_p))\n",
    "    return perf\n",
    "\n",
    "def get_perf_curve_ytrue(y_preds_p_A, y_preds_p_B):\n",
    "    perf = []\n",
    "    for i in range(max([len(y_pred) for y_pred in y_preds_p_A])):\n",
    "        y_pred_p = [pred[i] if i<len(pred) else pred[-1] for pred in y_preds_p_A]\n",
    "        y_pred_p_B = [pred[i] if i<len(pred) else pred[-1] for pred in y_preds_p_B]\n",
    "        y_pred_p.extend(y_pred_p_B)\n",
    "        perf.append(balanced_accuracy_score([*A_y_true[0], *B_y_true[0]], y_pred_p))\n",
    "    return perf"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "si6IjqPM_bP-",
    "outputId": "466e08a7-c83d-46f3-fc6a-b7c1d6223400"
   },
   "source": [
    "perf_1L_yhat_s = get_perf_curve_yhat(y_preds_p_1A_s, y_preds_p_1B_s)\n",
    "perf_1L_ytrue_s = get_perf_curve_ytrue(y_preds_p_1A_s, y_preds_p_1B_s)\n",
    "perf_1L_yhat_ns = get_perf_curve_yhat(y_preds_p_1A_ns, y_preds_p_1B_ns)\n",
    "perf_1L_ytrue_ns = get_perf_curve_ytrue(y_preds_p_1A_ns, y_preds_p_1B_ns)\n",
    "perf_1L_yhat_sam = get_perf_curve_yhat(y_preds_p_1A_sam, y_preds_p_1B_sam)\n",
    "perf_1L_ytrue_sam = get_perf_curve_ytrue(y_preds_p_1A_sam, y_preds_p_1B_sam)\n",
    "\n",
    "perf_2L_yhat_s = get_perf_curve_yhat(y_preds_p_2A_s, y_preds_p_2B_s)\n",
    "perf_2L_ytrue_s = get_perf_curve_ytrue(y_preds_p_2A_s, y_preds_p_2B_s)\n",
    "perf_2L_yhat_ns = get_perf_curve_yhat(y_preds_p_2A_ns, y_preds_p_2B_ns)\n",
    "perf_2L_ytrue_ns = get_perf_curve_ytrue(y_preds_p_2A_ns, y_preds_p_2B_ns)\n",
    "perf_2L_yhat_sam = get_perf_curve_yhat(y_preds_p_2A_sam, y_preds_p_2B_sam)\n",
    "perf_2L_ytrue_sam = get_perf_curve_ytrue(y_preds_p_2A_sam, y_preds_p_2B_sam)\n",
    "\n",
    "perf_3L_yhat_s = get_perf_curve_yhat(y_preds_p_3A_s, y_preds_p_3B_s)\n",
    "perf_3L_ytrue_s = get_perf_curve_ytrue(y_preds_p_3A_s, y_preds_p_3B_s)\n",
    "perf_3L_yhat_ns = get_perf_curve_yhat(y_preds_p_3A_ns, y_preds_p_3B_ns)\n",
    "perf_3L_ytrue_ns = get_perf_curve_ytrue(y_preds_p_3A_ns, y_preds_p_3B_ns)\n",
    "perf_3L_yhat_sam = get_perf_curve_yhat(y_preds_p_3A_sam, y_preds_p_3B_sam)\n",
    "perf_3L_ytrue_sam = get_perf_curve_ytrue(y_preds_p_3A_sam, y_preds_p_3B_sam)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7rdVXgZOE6r",
    "outputId": "4f907514-a5f7-480b-8665-8165a5bb0094"
   },
   "source": [
    "ess_yhat_1L_s = auc(range(len(perf_1L_yhat_s)), perf_1L_yhat_s)\n",
    "norm_ess_yhat_1L_s = ess_yhat_1L_s/len(perf_1L_yhat_s)\n",
    "ess_yhat_1L_ns = auc(range(len(perf_1L_yhat_ns)), perf_1L_yhat_ns)\n",
    "norm_ess_yhat_1L_ns = ess_yhat_1L_ns/len(perf_1L_yhat_ns)\n",
    "ess_yhat_1L_sam = auc(range(len(perf_1L_yhat_sam)), perf_1L_yhat_sam)\n",
    "norm_ess_yhat_1L_sam = ess_yhat_1L_sam/len(perf_1L_yhat_sam)\n",
    "\n",
    "ess_yhat_2L_s = auc(range(len(perf_2L_yhat_s)), perf_2L_yhat_s)\n",
    "norm_ess_yhat_2L_s = ess_yhat_2L_s/len(perf_2L_yhat_s)\n",
    "ess_yhat_2L_ns = auc(range(len(perf_2L_yhat_ns)), perf_2L_yhat_ns)\n",
    "norm_ess_yhat_2L_ns = ess_yhat_2L_ns/len(perf_2L_yhat_ns)\n",
    "ess_yhat_2L_sam = auc(range(len(perf_2L_yhat_sam)), perf_2L_yhat_sam)\n",
    "norm_ess_yhat_2L_sam = ess_yhat_2L_sam/len(perf_2L_yhat_sam)\n",
    "\n",
    "ess_yhat_3L_s = auc(range(len(perf_3L_yhat_s)), perf_3L_yhat_s)\n",
    "norm_ess_yhat_3L_s = ess_yhat_3L_s/len(perf_3L_yhat_s)\n",
    "ess_yhat_3L_ns = auc(range(len(perf_3L_yhat_ns)), perf_3L_yhat_ns)\n",
    "norm_ess_yhat_3L_ns = ess_yhat_3L_ns/len(perf_3L_yhat_ns)\n",
    "ess_yhat_3L_sam = auc(range(len(perf_3L_yhat_sam)), perf_3L_yhat_sam)\n",
    "norm_ess_yhat_3L_sam = ess_yhat_3L_sam/len(perf_3L_yhat_sam)\n",
    "\n",
    "print('ESS for TSA-S Explanations from SNN-1L: ', norm_ess_yhat_1L_s, ' +- ', conf_interval(norm_ess_yhat_1L_s, 180))\n",
    "print('ESS for TSA-NS Explanations from SNN-1L: ', norm_ess_yhat_1L_ns, ' +- ', conf_interval(norm_ess_yhat_1L_ns, 180))\n",
    "print('ESS for SAM Explanations from SNN-1L: ', norm_ess_yhat_1L_sam, ' +- ', conf_interval(norm_ess_yhat_1L_sam, 180))\n",
    "print('ESS for TSA-S Explanations from SNN-2L: ', norm_ess_yhat_2L_s, ' +- ', conf_interval(norm_ess_yhat_2L_s, 180))\n",
    "print('ESS for TSA-NS Explanations from SNN-2L: ', norm_ess_yhat_2L_ns, ' +- ', conf_interval(norm_ess_yhat_2L_ns, 180))\n",
    "print('ESS for SAM Explanations from SNN-2L: ', norm_ess_yhat_2L_sam, ' +- ', conf_interval(norm_ess_yhat_2L_sam, 180))\n",
    "print('ESS for TSA-S Explanations from SNN-3L: ', norm_ess_yhat_3L_s, ' +- ', conf_interval(norm_ess_yhat_3L_s, 180))\n",
    "print('ESS for TSA-NS Explanations from SNN-3L: ', norm_ess_yhat_3L_ns, ' +- ', conf_interval(norm_ess_yhat_3L_ns, 180))\n",
    "print('ESS for SAM Explanations from SNN-3L: ', norm_ess_yhat_3L_sam, ' +- ', conf_interval(norm_ess_yhat_3L_sam, 180))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "hjPgy1W82jgH",
    "outputId": "a5ebcd4e-ddde-4c55-c5c3-df6939ce70c9"
   },
   "source": [
    "fig = plt.figure(tight_layout=True, dpi=150, frameon=False, figsize=(15,5))\n",
    "gs = gridspec.GridSpec(1,3)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax1.plot(perf_1L_yhat_s)\n",
    "ax1.plot(perf_1L_yhat_ns)\n",
    "ax1.plot(perf_1L_yhat_sam, linestyle='dotted')\n",
    "ax1.set_ylim(0,1)\n",
    "ax1.set_ylabel('Balanced accuracy', fontdict={'size': 16})\n",
    "ax1.set_xlabel('Number of flipped segments', fontdict={'size': 16})\n",
    "ax1.legend(['TSA-S', 'TSA-NS', 'SAM'], prop={'size':13})\n",
    "ax1.set_title('Balanced accuracy of SNN-1L\\n with flipped feature segments\\n with regard to original model predictions')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "ax2.plot(perf_2L_yhat_s)\n",
    "ax2.plot(perf_2L_yhat_ns)\n",
    "ax2.plot(perf_2L_yhat_sam, linestyle='dotted')\n",
    "ax2.set_ylim(0,1)\n",
    "ax2.set_ylabel('Balanced accuracy', fontdict={'size': 16})\n",
    "ax2.set_xlabel('Number of flipped segments', fontdict={'size': 16})\n",
    "ax2.legend(['TSA-S', 'TSA-NS', 'SAM'], prop={'size':13})\n",
    "ax2.set_title('Balanced accuracy of SNN-2L \\n with flipped feature segments\\n with regard to original model predictions')\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,2])\n",
    "ax3.plot(perf_3L_yhat_s)\n",
    "ax3.plot(perf_3L_yhat_ns)\n",
    "ax3.plot(perf_3L_yhat_sam, linestyle='dotted')\n",
    "ax3.set_ylim(0,1)\n",
    "ax3.set_ylabel('Balanced accuracy', fontdict={'size': 16})\n",
    "ax3.set_xlabel('Number of flipped segments', fontdict={'size': 16})\n",
    "ax3.legend(['TSA-S', 'TSA-NS', 'SAM'], prop={'size':13})\n",
    "ax3.set_title('Balanced accuracy of SNN-3L \\n with flipped feature segments\\n with regard to original model predictions')\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AklT-y7azaeH",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "outputId": "1143b0b0-9da9-40cd-f910-cf39118fa9e5"
   },
   "source": [
    "fig = plt.figure(tight_layout=True, dpi=150, frameon=False, figsize=(15,5))\n",
    "gs = gridspec.GridSpec(1,3)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax1.plot(perf_1L_ytrue_s)\n",
    "ax1.plot(perf_1L_ytrue_ns)\n",
    "ax1.plot(perf_1L_ytrue_sam, linestyle='dotted')\n",
    "ax1.set_ylim(0,1)\n",
    "ax1.set_ylabel('Balanced accuracy', fontdict={'size': 16})\n",
    "ax1.set_xlabel('Number of flipped segments', fontdict={'size': 16})\n",
    "ax1.legend(['TSA-S', 'TSA-NS', 'SAM'], prop={'size':13})\n",
    "ax1.set_title('Balanced accuracy of SNN-1L\\n with flipped feature segments\\n with regard to ground truth')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "ax2.plot(perf_2L_ytrue_s)\n",
    "ax2.plot(perf_2L_ytrue_ns)\n",
    "ax2.plot(perf_2L_ytrue_sam, linestyle='dotted')\n",
    "ax2.set_ylim(0,1)\n",
    "ax2.set_ylabel('Balanced accuracy', fontdict={'size': 16})\n",
    "ax2.set_xlabel('Number of flipped segments', fontdict={'size': 16})\n",
    "ax2.legend(['TSA-S', 'TSA-NS', 'SAM'], prop={'size':13})\n",
    "ax2.set_title('Balanced accuracy of SNN-2L \\n with flipped feature segments\\n with regard to ground truth')\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,2])\n",
    "ax3.plot(perf_3L_ytrue_s)\n",
    "ax3.plot(perf_3L_ytrue_ns)\n",
    "ax3.plot(perf_3L_ytrue_sam, linestyle='dotted')\n",
    "ax3.set_ylim(0,1)\n",
    "ax3.set_ylabel('Balanced accuracy', fontdict={'size': 16})\n",
    "ax3.set_xlabel('Number of flipped segments', fontdict={'size': 16})\n",
    "ax3.legend(['TSA-S', 'TSA-NS', 'SAM'], prop={'size':13})\n",
    "ax3.set_title('Balanced accuracy of SNN-3L \\n with flipped feature segments\\n with regard to ground truth')\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9pZpiqzWMgd"
   },
   "source": [
    "# Continuity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWnNRaqBXO8M"
   },
   "source": [
    "### Read results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BRyBIwnYWN1W"
   },
   "source": [
    "def get_sens_score(nb_layer, expl_type):\n",
    "    A_score = load_obj('..\\\\continuity\\\\{}\\\\max_sensitivity_{}A.pkl'.format(expl_type, nb_layer))\n",
    "    B_score = load_obj('..\\\\continuity\\\\{}\\\\max_sensitivity_{}B.pkl'.format(expl_type, nb_layer))\n",
    "    score = max(A_score, B_score)\n",
    "    return score\n",
    "\n",
    "max_sensitivity_s = []\n",
    "max_sensitivity_ns = []\n",
    "max_sensitivity_sam = []\n",
    "\n",
    "for nb_layer in range(3):\n",
    "    max_sensitivity_s.append(get_sens_score(nb_layer, 's'))\n",
    "    max_sensitivity_ns.append(get_sens_score(nb_layer, 'ns'))\n",
    "    max_sensitivity_sam.append(get_sens_score(nb_layer, 'sam'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ04Wtw3XRVO"
   },
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gPqA7TGeXSrd"
   },
   "source": [
    "df_continuity = pd.DataFrame([max_sensitivity_s, max_sensitivity_ns, max_sensitivity_sam]).transpose()\n",
    "df_continuity\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBr4ke51P3Yx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Compactness"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load explanations. Compactness is then the sum of absolute attribution values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_explanations(expl_type, nb_layer, subject):\n",
    "    return load_obj('..\\\\{}\\\\{}L_explanations_{}_{}.pkl'.format(expl_type, nb_layer, subject))\n",
    "\n",
    "def compute_compactness(nb_layer, expl_type):\n",
    "    explanations = {**load_explanations(expl_type, nb_layer, 'A'), **load_explanations(expl_type, nb_layer, 'B')}\n",
    "    sum_absolute_attribution = 0\n",
    "    for key in explanations.keys():\n",
    "        sum_absolute_attribution += torch.sum(torch.abs(explanations[key][0]))\n",
    "    return sum_absolute_attribution/180"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compactness_s = []\n",
    "compactness_ns = []\n",
    "compactness_sam = []\n",
    "\n",
    "for nb_layer in range(3):\n",
    "    compactness_s.append(compute_compactness(nb_layer, 's'))\n",
    "    compactness_ns.append(compute_compactness(nb_layer, 'ns'))\n",
    "    compactness_sam.append(compute_compactness(nb_layer, 'sam'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute CI based on t-statistic and sample standard deviation for compactness\n",
    "def compute_sample_std(nb_layer, expl_type):\n",
    "    explanations = {**load_explanations(expl_type, nb_layer, 'A'), **load_explanations(expl_type, nb_layer, 'B')}\n",
    "    x_bar = compute_compactness(nb_layer, expl_type)\n",
    "    s = 0\n",
    "    for key in explanations.keys():\n",
    "        s += (torch.sum(torch.abs(explanations[key][0])) - x_bar)**2\n",
    "    s = s/179\n",
    "    return s\n",
    "\n",
    "def compute_95_ci(s, n):\n",
    "# t statistic is 1.97 for 180 dof and 95% ci\n",
    "    return 1.97*(s/(n**0.5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nb_layer in range(3):\n",
    "    for expl_type in ['s', 'ns', 'sam']:\n",
    "        s = compute_sample_std(nb_layer, expl_type)\n",
    "        ci = compute_95_ci(s, 180)\n",
    "        print('CI of SNN-{}L, {}:{}'.format(nb_layer, explanation_type, ci))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}