# -*- coding: utf-8 -*-
"""QuantitativeEvaluation - Playground.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kEoqH20gWonAYJ1YBCgkdKuYRq96FUW3

# Import libraries, models, data
"""

import os
import random
import sys

import numpy as np
import pandas as pd
from sklearn import metrics
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm

random.seed(123)
torch.manual_seed(123)
np.random.seed(123)

dtype = torch.float

# Check whether a GPU is available
if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

from ExplanationEvaluation import *

sys.path.insert(1, '../models')
from CoreSNN import *

# Load data & fixed variables
dataset = load_obj('/local/work/enguyen/data/dataset_max.pkl')
nb_inputs = 14
nb_outputs = 11

"""# Output-completeness

Output-completeness describes the sufficiency of the as highly attributing identified features/
part of the data for the model prediction. 
It is measured in the drop of model performance on the dataset when shuffling the unimportant features.

Definition here: attribution values > 0 of the feature segments are important.
"""


def identify_high_attribution(ranked_fs, epsilon):
    """
    filters the feature segments to those that are contributing more than epsilon
    :param ranked_fs: list of feature segments with their mean score
    :param epsilon: threshold for high attribution
    :return: feature segments that are >epsilon and the sensor dimensions that belong to them
    """
    high_attribution_segments = []
    attributing_sensors = []
    for fs in ranked_fs:
        if np.abs(fs[0]) > epsilon:
            high_attribution_segments.append(fs)
            if fs[1][0] not in attributing_sensors:
                attributing_sensors.append(fs[1][0])
    return high_attribution_segments, attributing_sensors


def get_time_segments(sensor, segments):
    """
    Function to extract the start timestamps and end timestamps sorted from early to late from a feature segment
    :param sensor: sensor dimension of segment (int)
    :param segments: feature segments to extract the timestamps for
    :returns: starts and ends in sorted lists
    """
    starts = []
    ends = []
    for _, segment in segments:
        if segment[0] == sensor:
            starts.append(segment[1])
            ends.append(segment[2])
    starts_sorted = [start for start, _ in sorted(zip(starts, ends))]
    ends_sorted = [end for _, end in sorted(zip(starts, ends))]
    return starts_sorted, ends_sorted


def perturb_background(attributing_sensors, X_spikes, segments):
    """
    Function to perturb only the background and not the high attributing segments randomly
    :param attributing_sensors: attributing sensors
    :param X_spikes: spiking input
    :param segments: highly attributing segments
    :returns: perturbed X_spikes
    """
    X_spikes_perturbed = X_spikes.to_dense()[0, :, :].t()
    for sensor in range(nb_inputs):
        if sensor in attributing_sensors:
            starts, ends = get_time_segments(sensor, segments)

            # identify the background data (non high attributing)
            background_data = [X_spikes_perturbed[sensor][:starts[0]]]
            for next_start, end in zip(starts[1:], ends[:-1]):
                background_data.append(X_spikes_perturbed[sensor][end:next_start])
            background_data.append(X_spikes_perturbed[sensor][ends[-1]:])
            background_data = torch.cat(background_data)

            # randomly permute the background
            shuffle_idx = torch.randperm(len(background_data))
            background_data = background_data[shuffle_idx]
            perturbed_data = [background_data[:starts[0]]]
            background_data = background_data[starts[0]:]
            for i in range(len(starts) - 1):
                perturbed_data.append(X_spikes_perturbed[sensor][starts[i]:ends[i]])
                perturbed_data.append(background_data[:(starts[i + 1] - ends[i])])
                background_data = background_data[(starts[i + 1] - ends[i]):]
            perturbed_data.append(X_spikes_perturbed[sensor][starts[-1]:ends[-1]])
            perturbed_data.append(background_data)
            perturbed_data = torch.cat(perturbed_data)
            X_spikes_perturbed[sensor] = perturbed_data
        else:
            shuffle_idx = torch.randperm(X_spikes.shape[1])
            X_spikes_perturbed[sensor] = X_spikes_perturbed[sensor][shuffle_idx]
    X_spikes_perturbed = X_spikes_perturbed.t()
    X_spikes_perturbed = torch.unsqueeze(X_spikes_perturbed, 0)
    return X_spikes_perturbed.to_sparse()


def output_completeness_score(nb_layers, X_data, y_data, model_explanations, epsilon, testset_t, y_true):
    """
    Computes the attribution sufficiency score as the balanced accuracy of the model's predictions on data with perturbed background and the ground truth
    :param nb_layers: number of layers of the SNN
    :param model_explanations: extracted explanations and X_spikes dictionary
    :param epsilon: threshold for high attribution
    :param testset_t: timestamps to test
    :returns: attribution sufficiency score and y_pred_bgperturbed
    """
    y_pred_bgperturbed = []
    y_preds = []
    model = initiate_model(nb_layers, 1)
    for t in tqdm(testset_t):
        start_t = t - 3600 if t >= 3600 else 0
        model.nb_steps = t - start_t
        model.max_time = t - start_t

        e, prediction = model_explanations[t]
        y_preds.append(prediction)

        # get the relevant part of the dataset, this is done for performance reasons
        X = {'times': X_data['times'][:, np.where((X_data['times'] >= start_t) & (X_data['times'] < t))[1]] - start_t,
             'units': X_data['units'][:, np.where((X_data['times'] >= start_t) & (X_data['times'] < t))[1]]}
        y = y_data[:, start_t:t]
        data_generator = sparse_data_generator_from_spikes(X, y, len(y), model.nb_steps, model.layer_sizes[0],
                                                           model.max_time, shuffle=False)
        X_spikes, _ = next(data_generator)

        fs = segment_features(e)
        fs_scores = rank_segments(e, fs)
        high_attribution_segments, attributing_sensors = identify_high_attribution(fs_scores, epsilon)
        X_spikes_perturbed = perturb_background(attributing_sensors, X_spikes, high_attribution_segments)

        y_pred_perturbed_here, _, _ = predict_from_spikes(model, X_spikes_perturbed)
        y_pred_bgperturbed.append(y_pred_perturbed_here[0][-1])
    score = metrics.balanced_accuracy_score(y_preds, y_pred_bgperturbed)
    return score, y_preds, y_pred_bgperturbed


def get_epsilons(max_attr):
    """
    Given all explanations of one SNN model across both data subjects, find the epsilons at 25,
    50 and 75% of the attribution range of positive attributions
    :param max_attr: maximum absolute attribution value
    :return:
    """
    epsilons = [(0), (0.25 * max_attr), (0.5 * max_attr), (0.75 * max_attr)]
    return epsilons


# concatenate A and B together for one network and find the max
# then test 0, 25% and 75% of the [0, max] interval as epsilons
A_testset_t = load_obj('../data/quantitative_test_t_A.pkl')
B_testset_t = load_obj('../data/quantitative_test_t_B.pkl')
A_y_true = dataset['y_test_A'][:, A_testset_t]
B_y_true = dataset['y_test_B'][:, B_testset_t]
expl_types = ['s', 'ns', 'sam']

with torch.no_grad():
    for expl_type in expl_types:
        # get epsilons
        max_attr = -50
        min_attr = 50
        for filename in os.listdir('../evaluation/{}'.format(expl_type)):
            f = '../evaluation/{}/{}'.format(expl_type, filename)
            if os.path.isfile(f):
                max_attr = max(max_attr, get_max_attr(f))
                min_attr = min(min_attr, get_min_attr(f))

        epsilons = get_epsilons(max(max_attr, np.abs(min_attr)))

        for nb_layer in range(3):
            # A
            model_explanations = load_obj('../evaluation/{}/{}L_explanations_A.pkl'.format(expl_type, nb_layer))
            for i, epsilon in enumerate(epsilons):
                oc_score = output_completeness_score(nb_layer, dataset['X_test_A'], dataset['y_test_A'],
                                                     model_explanations, epsilon, A_testset_t, A_y_true)
                save_obj(oc_score,
                         '../evaluation/output_completeness/{}/{}L_oc_A_epsilon{}.pkl'.format(expl_type, nb_layer, i))
            # B
            model_explanations = load_obj('../evaluation/{}/{}L_explanations_B.pkl'.format(expl_type, nb_layer))
            for i, epsilon in enumerate(epsilons):
                oc_score = output_completeness_score(nb_layer, dataset['X_test_B'], dataset['y_test_B'],
                                                     model_explanations, epsilon, B_testset_t, B_y_true)
                save_obj(oc_score,
                         '../evaluation/output_completeness/{}/{}L_oc_B_epsilon{}.pkl'.format(expl_type, nb_layer, i))
            print('Evaluation of output-completeness for {} explanations of SNN-{}L done!'.format(expl_type, nb_layer))
